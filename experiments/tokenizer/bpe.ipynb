{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ebb19a2",
   "metadata": {},
   "source": [
    "## Basic Byte-Pair-Encoding(BPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a735fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded example:  omen\n",
      "Encoded example:  [84, 104, 258, 84]\n"
     ]
    }
   ],
   "source": [
    "class BasicBPE:\n",
    "\n",
    "    def __init__(self, vocab_size: int):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = {idx: bytes([idx]) for idx in range(256)} # BPE definition\n",
    "        self.merges = dict()\n",
    "\n",
    "    def train(self, text: str):\n",
    "        \"\"\"\n",
    "        1. Encode text into UTF-8 format.\n",
    "        2. Search for pairs and merges.\n",
    "        3. Apply merges and repeat 2. until end condition.\n",
    "        \"\"\"\n",
    "\n",
    "        tokens = text.encode('utf-8')\n",
    "        tokens = list(map(int, tokens))\n",
    "\n",
    "        while len(self.merges) + 256 <= self.vocab_size:\n",
    "            # compute tokens stats\n",
    "            tokens_stats = compute_pair_of_tokens_stats(tokens)\n",
    "            # get most common pair of tokens\n",
    "            most_common_pair = max(tokens_stats, key=tokens_stats.get)\n",
    "            new_token_id = len(self.merges) + 1 + 256 # UTF-8 has 256 ints\n",
    "            # save change\n",
    "            self.merges[most_common_pair] = new_token_id\n",
    "            # apply change\n",
    "            tokens = merge_pair(tokens, pair=most_common_pair, idx=new_token_id)\n",
    "\n",
    "        # update tokens map\n",
    "        for (p0, p1), idx in self.merges.items():\n",
    "            self.vocab[idx] = self.vocab[p0] + self.vocab[p1]\n",
    "    \n",
    "    def encode(self, text: str):\n",
    "        \"\"\"\n",
    "        1. Compute UTF-8 encoding of text.\n",
    "        2. Apply merges to convert UTF-8 encoding to BPE encoding.\n",
    "        \"\"\"\n",
    "        tokens = list(text.encode('utf-8'))\n",
    "        while len(tokens) >= 2:\n",
    "            stats = compute_pair_of_tokens_stats(tokens)\n",
    "            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            if pair not in self.merges:\n",
    "                break # nothing else can be merged\n",
    "            idx = self.merges[pair]\n",
    "            tokens = merge_pair(tokens, pair, idx)\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        \"\"\"\n",
    "        1. Convert tokens from BPE enconding to UTF-8 encoding.\n",
    "        2. Decode UTF-8 to text.\n",
    "        \"\"\"\n",
    "        text = b\"\".join(self.vocab[x] for x in tokens)\n",
    "        text = text.decode(\"utf-8\", errors=\"replace\")\n",
    "        return text\n",
    "\n",
    "\n",
    "def compute_pair_of_tokens_stats(tokens):\n",
    "\n",
    "    info = {}\n",
    "    for pair in zip(tokens, tokens[1:]):\n",
    "        if pair not in info.keys():\n",
    "            info[pair] = 1\n",
    "        else:\n",
    "            info[pair] += 1\n",
    "    return info\n",
    "\n",
    "def merge_pair(tokens, pair, idx):\n",
    "    new_list = []\n",
    "    i = 0\n",
    "    total_tokens = len(tokens)\n",
    "    while i < total_tokens:\n",
    "        if i < total_tokens - 1 and tokens[i] == pair[0] and tokens[i+1] == pair[1]:\n",
    "            new_list.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_list.append(tokens[i])\n",
    "            i += 1\n",
    "    return new_list\n",
    "\n",
    "text = \"The Tokenizer is a necessary and pervasive component of Large Language Models (LLMs), where it translates between strings and tokens (text chunks). Tokenizers are a completely separate stage of the LLM pipeline: they have their own training sets, training algorithms (Byte Pair Encoding), and after training implement two fundamental functions: encode() from strings to tokens, and decode() back from tokens to strings. In this lecture we build from scratch the Tokenizer used in the GPT series from OpenAI. In the process, we will see that a lot of weird behaviors and problems of LLMs actually trace back to tokenization. We'll go through a number of these issues, discuss why tokenization is at fault, and why someone out there ideally finds a way to delete this stage entirely.\"\n",
    "\n",
    "bpe = BasicBPE(vocab_size=300)\n",
    "bpe.train(text)\n",
    "decoding_example = bpe.decode([270,260])\n",
    "print(\"Decoded example: \", decoding_example)\n",
    "encoding_example = bpe.encode(text[:5])\n",
    "print(\"Encoded example: \", encoding_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "68dcb934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(bpe.decode(bpe.encode(\"hello world\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "606b4486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens using UFT-8: 781\n",
      "Number of tokens using BPE: 481\n",
      "Compression ratio: 162.37006237006239\n"
     ]
    }
   ],
   "source": [
    "tokens_level_utf8 = text.encode(\"utf-8\")\n",
    "tokens_level_bpe = bpe.encode(text)\n",
    "print(f\"Number of tokens using UFT-8: {len(tokens_level_utf8)}\")\n",
    "print(f\"Number of tokens using BPE: {len(tokens_level_bpe)}\")\n",
    "print(f\"Compression ratio: {len(tokens_level_utf8)/len(tokens_level_bpe)*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11b6ed5",
   "metadata": {},
   "source": [
    "# Regex Byte-Pair-Encoding (RegexBPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7438944c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[103, 108, 111, 98, 97, 108]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bytearray(\"global\", encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c9a6a44d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[103, 108, 111, 98, 97, 108]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\"global\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2f2674",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
